{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce784aad-b47c-443a-9f05-9d2820e86757",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##########\n",
    "# PART 1 #\n",
    "##########\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9dd8044-e6da-4363-b313-2b3a3f35e6de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /root/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package maxent_ne_chunker to\n[nltk_data]     /root/nltk_data...\n[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n[nltk_data] Downloading package words to /root/nltk_data...\n[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[106]: True"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcfc562b-60c3-4b16-88ec-331548416763",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_named_entities(text):\n",
    "    words = word_tokenize(text)\n",
    "    tagged = pos_tag(words)\n",
    "    return [\" \".join([c[0] for c in chunk]) for chunk in ne_chunk(tagged) if hasattr(chunk, 'label')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d661a04-e331-47de-b8e0-ade80045f36d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahab: 406\nStubb: 236\nQueequeg: 209\nPequod: 158\nStarbuck: 148\nFlask: 91\nJonah: 75\nSperm Whale: 70\nNantucket: 66\nCaptain: 64\n"
     ]
    }
   ],
   "source": [
    "# Import Gutenberg text file into RDD\n",
    "text_rdd = sc.textFile('dbfs:/FileStore/tables/moby_dick.txt')\n",
    "\n",
    "# Extract named entities from the RDD\n",
    "named_entities_rdd = text_rdd.flatMap(extract_named_entities)\n",
    "\n",
    "# MapReduce to count named entities and sort the result in descending order of count\n",
    "sorted_named_entities_counts = named_entities_rdd.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y).sortBy(lambda x: -x[1])\n",
    "\n",
    "# Display the top 10 named entities and their counts\n",
    "for entity, count in sorted_named_entities_counts.take(10):\n",
    "    print(f'{entity}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e03c4432-b99b-4523-84b4-413bb4d4a957",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##########\n",
    "# PART 2 #\n",
    "##########\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import string\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0a40b2d-4896-4660-91fb-420a6a60e062",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f7645a5-e0d2-4ed5-8b83-aeee5c0b7dc8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Load Data\n",
    "plot_df = spark.read.csv('dbfs:/FileStore/tables/plot_summaries.txt', sep=\"\\t\")\n",
    "new_plot_column_names = [\"movie_id\", \"plot_summary\"]\n",
    "for old_col, new_col in zip(plot_df.columns, new_plot_column_names):\n",
    "    plot_df = plot_df.withColumnRenamed(old_col, new_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f18424b0-6d6f-426a-ad87-75a03b5cc736",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n|movie_id|        plot_summary|\n+--------+--------------------+\n|23890098|Shlykov, a hard-w...|\n|31186339|The nation of Pan...|\n|20663735|Poovalli Induchoo...|\n| 2231378|The Lemon Drop Ki...|\n|  595909|Seventh-day Adven...|\n| 5272176|The president is ...|\n| 1952976|{{plot}} The film...|\n|24225279|The story begins ...|\n| 2462689|Infuriated at bei...|\n|20532852|A line of people ...|\n|15401493|Lola  attempts to...|\n|18188932|Milan and Goran a...|\n| 2940516|Bumbling pirate c...|\n| 1335380|The film is based...|\n| 1480747|{{plot}} Followin...|\n|24448645|Despite Lucy's re...|\n|15072401|Alan Colby, heir ...|\n| 4018288|Debbie's favorite...|\n| 4596602|Ashes to Ashes is...|\n|15224586|The film follows ...|\n+--------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "plot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4af367a-52af-44fa-a987-85b9ceabadc4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1.5: Create mapping of movie ids to movie names\n",
    "movie_df = spark.read.csv(\"dbfs:/FileStore/tables/movie_metadata.tsv\", sep=\"\\t\")\n",
    "new_movie_column_names = [\n",
    "    \"movie_id\",\n",
    "    \"freebase_id\",\n",
    "    \"movie_name\",\n",
    "    \"release_date\",\n",
    "    \"revenue\",\n",
    "    \"runtime\",\n",
    "    \"languages\",\n",
    "    \"countries\",\n",
    "    \"genres\",\n",
    "]\n",
    "for old_col, new_col in zip(movie_df.columns, new_movie_column_names):\n",
    "    movie_df = movie_df.withColumnRenamed(old_col, new_col)\n",
    "    \n",
    "movie_plot_df = plot_df.join(\n",
    "    movie_df.select(\"movie_id\", \"movie_name\"), on=\"movie_id\", how=\"inner\"\n",
    ")\n",
    "movie_id_name_dict = movie_df.select(\"movie_id\", \"movie_name\").rdd.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c7a95f3-2816-4fe5-aaec-1eef3346f486",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove punctuation and convert to lowercase\n",
    "    tokens = [token.translate(str.maketrans('', '', string.punctuation)).lower() for token in tokens]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Remove words under 2 letters\n",
    "    tokens = [token for token in tokens if len(token) > 2]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ff9f70a-1181-4a1d-8d41-687fb8347501",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n|movie_id|        plot_summary|\n+--------+--------------------+\n|23890098|shlykov hardworki...|\n|31186339|nation panem cons...|\n|20663735|poovalli induchoo...|\n| 2231378|lemon drop kid ne...|\n|  595909|seventhday advent...|\n| 5272176|president way giv...|\n| 1952976|plot film opens 1...|\n|24225279|story begins hann...|\n| 2462689|infuriated told w...|\n|20532852|line people drool...|\n|15401493|lola attempts gai...|\n|18188932|milan goran two c...|\n| 2940516|bumbling pirate c...|\n| 1335380|film based events...|\n| 1480747|plot following su...|\n|24448645|despite lucy rese...|\n|15072401|alan colby heir v...|\n| 4018288|debbie favorite b...|\n| 4596602|ashes ashes set l...|\n|15224586|film follows expe...|\n+--------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Preprocessing\n",
    "# Apply the preprocessing UDF to the plot summary column\n",
    "plot_df = plot_df.withColumn(\n",
    "    \"plot_summary\", udf(preprocess_text, StringType())(plot_df[\"plot_summary\"])\n",
    ")\n",
    "plot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e876327-a7b3-49db-9ad8-dceb95d18b68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "N = plot_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03737003-371f-4d4b-8c35-0967a81fe557",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: Calculate TF-IDF\n",
    "tf = (\n",
    "    plot_df.rdd.flatMap(\n",
    "        lambda x: [((x[0], i, len(x[1].split())), 1) for i in x[1].split()]\n",
    "    )\n",
    "    .reduceByKey(lambda x, y: x + y)\n",
    "    .map(lambda x: (x[0][1], (x[0][0], x[1] / x[0][2])))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69074896-03cf-457a-8966-e7caa8c95be8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "idf = (\n",
    "    tf.map(lambda x: (x[0], 1))\n",
    "    .reduceByKey(lambda x, y: x + y)\n",
    "    .map(lambda x: (x[0], math.log10(N / x[1])))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e390f3bb-3189-4102-ad88-9672d64647d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "idf_dict = idf.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98e452c2-aadd-46d0-a71c-9ad4987502bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tf_idf = (\n",
    "    tf.join(idf)\n",
    "    .map(lambda x: (x[1][0][0], x[0], x[1][0][1], x[1][1], x[1][0][1] * x[1][1]))\n",
    "    .sortBy(lambda x: x[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2d95e5f-4493-45f6-8a86-ecac7e532ba6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------------+------------------+--------------------+\n|movie_id|       term|                  tf|               idf|              tf-idf|\n+--------+-----------+--------------------+------------------+--------------------+\n|10000053|    decides|0.003215434083601286| 0.759701209018187|0.002442769160830...|\n|10000053|     starts|0.006430868167202572|0.9384273450261305|0.006034902540360968|\n|10000053| threatened|0.003215434083601286|1.9062426616547292|0.006129397625899451|\n|10000053|   murdered|0.003215434083601286|1.3580580511096214|0.004366746145047014|\n|10000053| eventually|0.003215434083601286|0.8083080959140502|0.002599061401652...|\n|10000053|     spends|0.003215434083601286|1.5917735084353657|0.005118242792396675|\n|10000053|opportunity|0.003215434083601286| 1.577571878532336|0.005072578387563782|\n|10000053|      shock|0.003215434083601286|1.7227694489764485|0.005539451604425879|\n|10000053|    waiting|0.003215434083601286|1.3604055945656068|0.004374294516288125|\n|10000053|     shoots|0.003215434083601286| 1.230551589041905|0.003956757521035064|\n|10000053|        shy|0.003215434083601286| 2.027611458297571|0.006519650991310517|\n|10000053|     period|0.003215434083601286|1.7727037532845116|0.005700012068438944|\n|10000053|        bid|0.003215434083601286| 2.235466857957307|   0.007187996327837|\n|10000053|     nights|0.003215434083601286|2.2879454714560814|0.007356737850341097|\n|10000053|   advances|0.003215434083601286|1.8168422504254182|0.005841936496544753|\n|10000053|husbandtobe|0.003215434083601286| 3.626401965060686| 0.01166045647929481|\n|10000053|     trader| 0.00964630225080386|2.6351758893681914|0.025419703112876446|\n|10000053|       lion|0.003215434083601286|2.2820096913755754|0.007337651740757477|\n|10000053|     bailed|0.003215434083601286|  2.79389305235445|0.008983578946477331|\n|10000053|fosterchild|0.006430868167202572| 4.626401965060686|0.029751781125792194|\n+--------+-----------+--------------------+------------------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "tf_idf.toDF([\"movie_id\",\"term\",\"tf\",\"idf\",\"tf-idf\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e4410e5-e7c1-4df7-8bf9-b2697c352b89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tf_idf_dict = tf_idf.map(lambda x: ((x[0], x[1]), x[4])).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "514619d4-5898-48ff-a6ee-a993a25d6e3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "movie_ids = tf_idf.map(lambda x: x[0]).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c416b7f9-191a-4dd3-b937-15b8e82f6f9e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def print_search_results(query, results):\n",
    "    if len(results) == 0:\n",
    "        print(f\"No results found for '{query}'\")\n",
    "    else:\n",
    "        print(f\"Search results for '{query}':\")\n",
    "        for i, result in enumerate(results, start=1):\n",
    "            print(f\"{i}. {result}\")\n",
    "    print('----------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72e6cfcf-c3d5-40f4-8903-96c2c949b65c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def tf_idf_terms(terms):\n",
    "    tf = sorted(map(lambda x: (x[0], x[1] / len(terms)), Counter(terms).items()))\n",
    "    idf = sorted([(term, idf_dict.get(term, 0)) for term in terms])\n",
    "    tf_idf = [(term, tf * idf) for (term, tf), (term, idf) in zip(tf, idf)]\n",
    "    return tf_idf\n",
    "\n",
    "def compute_cosine_similarity(terms, movie_id):\n",
    "    terms_tf_idf = tf_idf_terms(terms)\n",
    "    movies_tf_idf = [(term[0], tf_idf_dict.get((movie_id, term[0]), 0)) for term in terms_tf_idf]\n",
    "    terms_vals = [tf_idf for term, tf_idf in terms_tf_idf]\n",
    "    movie_vals = [tf_idf for term, tf_idf in movies_tf_idf]\n",
    "\n",
    "    # # Compute dot product\n",
    "    dot_product = dot(terms_vals, movie_vals)\n",
    "\n",
    "    # # Compute magnitudes\n",
    "    magnitude = norm(terms_vals) * norm(movie_vals)\n",
    "\n",
    "    return dot_product / magnitude if magnitude != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd7f8bd1-106d-474a-8a42-3f4c4c60d499",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search results for 'action':\n1. Giri\n2. Tiger\n3. Ganga Ki Kasam\n4. Hitler\n5. Numbri Aadmi\n6. Kaalia\n7. Anyay Abichar\n8. Crayon Shin-chan: The Storm Called: Operation Golden Spy\n9. Cracker Jack\n10. Kranti Kshetra\n----------------------------------------\nSearch results for 'romance':\n1. Ranmuthu Duwa\n2. L'Amour\n3. Office Lady Love Juice\n4. That Dangerous Age\n5. Lovers and Liars\n6. Moment by Moment\n7. Hollywood Dreams\n8. Barefooted Youth\n9. Coming Back\n10. Dance Pretty Lady\n----------------------------------------\nSearch results for 'Funny movie with actions scenes':\n1. The Perfect Holiday\n2. Phobia 2\n3. Le Distrait\n4. My Name Is Joe\n5. Full Grown Men\n6. Delta Farce\n7. Taking Five\n8. Meet the Fockers\n9. Trail of the Pink Panther\n10. Modalasala\n----------------------------------------\nSearch results for 'Bloody movie with vampires':\n1. Blood of Dracula's Castle\n2. The Blood Spattered Bride\n3. Goke, Body Snatcher from Hell\n4. The Addiction\n5. The Return of Count Yorga\n6. 30 Days of Night: Blood Trails\n7. Bordello of Blood\n8. The Breed\n9. The Monster Club\n10. I Am Virgin\n----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Queries\n",
    "queries = [\n",
    "    \"action\",\n",
    "    \"romance\",\n",
    "    \"Funny movie with actions scenes\",\n",
    "    \"Bloody movie with vampires\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    terms = preprocess_text(query).split()\n",
    "    if len(terms) == 1:\n",
    "        term = terms[0]\n",
    "        top_movies = tf_idf.filter(lambda x: x[1] == term).sortBy(lambda x: x[4], ascending=False).take(10)\n",
    "        top_movie_names = [movie_id_name_dict[top_movie_id] for top_movie_id, *_ in top_movies]\n",
    "    else:\n",
    "        top_movies = movie_ids.map(lambda x: (x, compute_cosine_similarity(terms, x))).sortBy(lambda x: x[1], ascending=False).take(10)\n",
    "        top_movie_names = [movie_id_name_dict[top_movie_id] for top_movie_id, *_ in top_movies]\n",
    "\n",
    "    print_search_results(query, top_movie_names)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CS_6350_Assignment_1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
