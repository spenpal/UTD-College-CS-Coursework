{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import dayofweek, col\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA\n",
    "\n",
    "You can download the data at [Kaggle site](https://www.kaggle.com/datasets/patrickzel/flight-delay-and-cancellation-dataset-2019-2023?select=2021.csv) or through the provided S3 bucket location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"FlightDelayPrediction\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"2021.csv\"\n",
    "data_2021 = spark.read.csv(file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"DEP_DELAY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2021.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2021.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2021.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_col in data_2021.columns:\n",
    "    missing_count = data_2021.filter(data_2021[df_col].isNull()).count()\n",
    "    print(f\"{df_col}: {missing_count} missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING / FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort data by FL_DATE column in ascending order\n",
    "preprocessed_data_2021 = data_2021.sort(\"FL_DATE\")\n",
    "\n",
    "# Remove rows that have cancelled flights\n",
    "preprocessed_data_2021 = preprocessed_data_2021.filter(data_2021.CANCELLED == 0)\n",
    "\n",
    "# Add column for day of the week, based on FL_DATE\n",
    "preprocessed_data_2021 = preprocessed_data_2021.withColumn(\"DAY_OF_WEEK\", dayofweek(col(\"FL_DATE\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep these columns in a new dataframe (Domain Knowledge Feature Selection)\n",
    "preprocessed_data_2021 = preprocessed_data_2021.select(\"FL_YEAR\", \"FL_MONTH\", \"FL_DAY\", \"DAY_OF_WEEK\", \"AIRLINE_CODE\", \"ORIGIN\", \"DEST\", \"CRS_DEP_TIME\", target_col)\n",
    "\n",
    "preprocessed_data_2021 = preprocessed_data_2021.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data_2021.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical columns to OneHotEncoded vector\n",
    "indexer = StringIndexer(inputCols=['AIRLINE_CODE', 'ORIGIN', 'DEST'], outputCols=['INDEXED_AIRLINE_CODE', 'INDEXED_ORIGIN', 'INDEXED_DEST'])\n",
    "preprocessed_data_2021 = indexer.fit(preprocessed_data_2021).transform(preprocessed_data_2021)\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=['INDEXED_AIRLINE_CODE', 'INDEXED_ORIGIN', 'INDEXED_DEST'], outputCols=['ONEHOT_AIRLINE_CODE', 'ONEHOT_ORIGIN', 'ONEHOT_DEST'])\n",
    "preprocessed_data_2021 = encoder.fit(preprocessed_data_2021).transform(preprocessed_data_2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep these onehot encoded columns only\n",
    "preprocessed_data_2021 = preprocessed_data_2021.select(\"FL_YEAR\", \"FL_MONTH\", \"FL_DAY\", \"DAY_OF_WEEK\", 'ONEHOT_AIRLINE_CODE', 'ONEHOT_ORIGIN', 'ONEHOT_DEST', target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only first 10,000 rows, due to limited CPU power. If you have a better CPU or running this on AWS, you can comment this line out.\n",
    "preprocessed_data_2021 = preprocessed_data_2021.limit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data_2021.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data_2021.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization\n",
    "feature_cols = list(set(preprocessed_data_2021.columns) - set([target_col])) # All columns except ARR_DELAY (target variable)\n",
    "vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "vectorized_data_2021 = vector_assembler.transform(preprocessed_data_2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data_2021.select(\"features\", target_col).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization\n",
    "scaler = StandardScaler(withMean=True, withStd=True, inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "scaled_data_2021 = scaler.fit(vectorized_data_2021).transform(vectorized_data_2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data_2021.select(\"scaled_features\", target_col).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN/TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train/test split percentage\n",
    "train_percent = 0.6\n",
    "val_percent = 0.2\n",
    "test_percent = 0.2\n",
    "\n",
    "# Calculate the split indices\n",
    "count = scaled_data_2021.count()\n",
    "train_index = int(train_percent * count)\n",
    "val_index = train_index + int(val_percent * count)\n",
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "train_data = scaled_data_2021.limit(train_index)\n",
    "val_data = scaled_data_2021.limit(val_index).subtract(train_data)\n",
    "test_data = scaled_data_2021.subtract(train_data).subtract(val_data)\n",
    "\n",
    "# Extract train, validation, test sets from the dataset\n",
    "X_train, y_train = train_data.select('scaled_features'), train_data.select(target_col)\n",
    "X_val, y_val = val_data.select('scaled_features'), val_data.select(target_col)\n",
    "X_test, y_test = test_data.select('scaled_features'), test_data.select(target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL BUILDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.lr = learning_rate\n",
    "        self.clip_value = 0.01\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.input_hidden_weights = np.random.randn(input_size, hidden_size)\n",
    "        self.hidden_output_weights = np.random.randn(hidden_size, output_size)\n",
    "        starting_bias = 0.1\n",
    "        self.hidden_bias = np.array([starting_bias] * hidden_size)\n",
    "        self.output_bias = np.array([starting_bias] * output_size)\n",
    "    \n",
    "    def leaky_relu(self, x):\n",
    "        return np.where(x > 0, x, 0.01 * x)\n",
    "    \n",
    "    def clip_gradients(self, gradients):\n",
    "        return np.clip(gradients, -self.clip_value, self.clip_value)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.hidden_state = self.leaky_relu(x.dot(self.input_hidden_weights) + self.hidden_bias) \n",
    "        output = self.hidden_state.dot(self.hidden_output_weights) + self.output_bias \n",
    "        return output\n",
    "\n",
    "    def backward(self, x, output, target):\n",
    "        loss_gradient = 2 * (output - target)  # Mean Squared Error loss gradient\n",
    "        hidden_state_gradient = self.hidden_output_weights.dot(loss_gradient)\n",
    "        \n",
    "        # Clip gradients before updating weights\n",
    "        hidden_state_gradient_clipped = self.clip_gradients(hidden_state_gradient)\n",
    "        loss_gradient_clipped = self.clip_gradients(loss_gradient)\n",
    "    \n",
    "        self.input_hidden_weights -= self.lr * np.outer(x, hidden_state_gradient_clipped)\n",
    "        self.hidden_output_weights -= self.lr * np.outer(self.hidden_state, loss_gradient_clipped)\n",
    "        self.hidden_bias -= self.lr * hidden_state_gradient_clipped\n",
    "        self.output_bias -= self.lr * loss_gradient_clipped\n",
    "        \n",
    "    def predict(self, x):\n",
    "        self.hidden_state = self.leaky_relu(x.dot(self.input_hidden_weights) + self.hidden_bias)\n",
    "        output = self.hidden_state.dot(self.hidden_output_weights) + self.output_bias\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean(np.square(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_np, y_train_np = X_train.collect(), y_train.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_np, y_val_np = X_val.collect(), y_val.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "hidden_size = 1\n",
    "\n",
    "input_size = len(X_train_np[0][0])\n",
    "output_size = 1\n",
    "model = RNN(input_size, hidden_size, output_size, learning_rate)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch + 1}:')\n",
    "    \n",
    "    # Training evaluation\n",
    "    train_loss = 0\n",
    "    for i in range(len(X_train_np)):\n",
    "        input_train = np.array(X_train_np[i][0])\n",
    "        target_train = y_train_np[i][0]\n",
    "        output_train = model.forward(input_train)\n",
    "        model.backward(input_train, output_train, target_train)\n",
    "        train_loss += mean_squared_error(target_train, output_train)\n",
    "    avg_train_loss = train_loss / len(X_train_np)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    print(f'Train Loss: {avg_train_loss}', end=' | ')\n",
    "    \n",
    "    # Validation evaluation\n",
    "    val_loss = 0\n",
    "    for i in range(len(X_val_np)):\n",
    "        input_val = np.array(X_val_np[i][0])\n",
    "        target_val = y_val_np[i][0]\n",
    "        output_val = model.forward(input_val)\n",
    "        val_loss += mean_squared_error(target_val, output_val)\n",
    "    avg_val_loss = val_loss / len(X_val_np)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    print(f'Validation Loss: {avg_val_loss}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(train_loss, val_loss, hidden_size):\n",
    "    # Create an array for the number of epochs, e.g., [1, 2, 3, ...]\n",
    "    epochs = np.arange(1, len(train_loss) + 1)\n",
    "\n",
    "    # Plot training accuracy and validation accuracy on the same graph\n",
    "    plt.plot(epochs, train_loss, label='Training Loss', marker='o', linestyle='-')\n",
    "    plt.plot(epochs, val_loss, label='Validation Loss', marker='o', linestyle='-')\n",
    "\n",
    "    # Set axis labels and a legend\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.xticks(np.arange(1, len(train_loss) + 1, step=1))\n",
    "    plt.title(f'h = {hidden_size} | Training and Validation Loss (by Epoch)')\n",
    "    plt.show()\n",
    "    \n",
    "plot_loss(train_losses, val_losses, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_np, y_test_np = X_test.collect(), y_test.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on a new set of data\n",
    "preds = []\n",
    "\n",
    "for i in range(len(X_test_np)):\n",
    "    input = np.array(X_test_np[i][0])\n",
    "    output = model.predict(input)\n",
    "    preds.append(output)\n",
    "\n",
    "# Convert predictions to a NumPy array\n",
    "preds = np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_value = mean_squared_error(y_test_np, preds)\n",
    "print(f\"Test Mean Squared Error: {mse_value}\")\n",
    "print(f\"Test Root Mean Squared Error: {np.sqrt(mse_value)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
